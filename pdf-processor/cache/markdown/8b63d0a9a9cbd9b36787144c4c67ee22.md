# ğŸ“ Research Paper Analysis

# ğŸ“š Table of Contents

- ğŸ“„ Document Analysis
  - Document Metadata
  - ğŸ¤– Claude Code Integration Notes
  - ğŸ”¢ Mathematical Content Summary
  - ğŸ“‘ Page 1
- **High-Degrees-of-Freedom Dynamic Neural Fields**
- **?**
- arXiv:2310.03624v2  [cs.CV]  19 Apr 2024
  - ğŸ“‘ Page 2
  - ğŸ“‘ Page 3
  - ğŸ“‘ Page 4
  - ğŸ“‘ Page 5
  - ğŸ“‘ Page 6
  - ğŸ“‘ Page 7

# ğŸ“„ Document Analysis

## Document Metadata
- **Title**: High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning
- **Author**: Lennart Schulze, Hod Lipson
- **Subject**: High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning
- **Creator**: LaTeX with hyperref
- **Pages**: 7
- **Format**: PDF 1.5

## ğŸ¤– Claude Code Integration Notes
- This document has been optimized for Claude Code analysis
- Mathematical formulas are preserved in LaTeX format
- Code blocks and algorithms are clearly marked
- Tables and figures are structurally annotated

---

## ğŸ”¢ Mathematical Content Summary
1. `Î´`
2. `Î¦`
3. `Î±`
4. `Î `
5. `Ïƒ`

---

## ğŸ“‘ Page 1

# **High-Degrees-of-Freedom Dynamic Neural Fields**
**for Robot Self-Modeling and Motion Planning**

Lennart Schulze 1 and Hod Lipson 2

$$
***Abstract*** **â€” A robot self-model is a task-agnostic representa-**
**tion of the robotâ€™s physical morphology that can be used for**
**motion planning tasks in the absence of a classical geometric**
**kinematic model. In particular, when the latter is hard to en-**
**gineer or the robotâ€™s kinematics change unexpectedly, human-**
**free self-modeling is a necessary feature of truly autonomous**
**agents. In this work, we leverage neural fields to allow a robot**
**to self-model its kinematics as a neural-implicit query model**
**learned only from 2D images annotated with camera poses and**
**configurations. This enables significantly greater applicability**
**than existing approaches which have been dependent on depth**
**images or geometry knowledge. To this end, alongside a curric-**
**ular data sampling strategy, we propose a new encoder-based**
**neural density field architecture for dynamic object-centric**
**scenes conditioned on high numbers of degrees of freedom**
**(DOFs). In a 7-DOF robot test setup, the learned self-model**
**achieves a Chamfer-L2 distance of 2% of the robotâ€™s workspace**
**dimension. We demonstrate the capabilities of this model on**
**motion planning tasks as an exemplary downstream application.**
$$

I. I NTRODUCTION

$$
Neural fields paired with differentiable rendering allow
learning accurate 3D scene information from pose-annotated
2D images. This is achieved by overfitting a neural network
to the scene observed from multiple camera views using a
photometric reconstruction loss [ 1 ]. After training, the model
can be used to render realistic images of the scene from novel
camera views. Due to the importance of scene representations
in robotics, neural field extensions have evolved focusing on
use cases in this area. While most of these approaches [ 2 ,
3 , 4 , 5 ] use neural fields to capture and utilize information
about the robotâ€™s environment, such as for reconstruction,
navigation, or localization tasks, here we propose to learn
neural fields to represent - and control - the robot.
We solve the task of robot self-modeling, the (robotâ€™s)
ability to acquire a representation of the robotâ€™s kinematics
from observing its behavior without human interference.
Similar to a mental image of oneself, self-models can
continually be updated to reflect the state of the robot.
This renders them advantageous over classical geometric
kinematic models, which are usually engineered once, may
be mismatched to the current state of the robot, and are
unavailable for unknown robots [ 6 ]. For these reasons,
learning-based approaches to robot self-modeling emerged.
Despite functional, a major drawback is their dependence
on supervised samples or, in the self-supervised case, depth
annotations in the training distribution. These requirements
$$

1 Department of Computer Science, Columbia University, New York, NY 10027, USA. lennart.schulze@columbia.edu 2 Department of Mechanical Engineering, Columbia University, New York, NY 10027, USA. hod.lipson@columbia.edu

# **?**

**Figure 1**: No caption
*Dimensions: 800x800 pixels*

**Figure 2**: No caption
*Dimensions: 786x860 pixels*

**Figure 3**: No caption
*Dimensions: 800x800 pixels*

**Figure 4**: No caption
*Dimensions: 800x800 pixels*

**Figure 5**: No caption
*Dimensions: 800x800 pixels*

$$
**Unknown / irregular**
$$

$$
**robot**
$$

$$
**Unavailable**
**kinematic model**
$$

$$
**Annotated RGB-only images**
$$

$$
**of rotating base and joints**
$$

**Figure 6**: No caption
*Dimensions: 800x800 pixels*

ğœ½ !

ğš«ğœ½?

**Figure 7**: No caption
*Dimensions: 807x819 pixels*

ğœƒ

ğœƒ

ğœƒ

ğœƒ

ğœƒ

ğœƒ

ğœƒ

$$
**Neural-field-encoded**
$$

$$
**kinematics**
$$

$$
**Training dynamic neural**
$$

$$
**field self-supervised**
$$

â€¦

**Figure 8**: No caption
*Dimensions: 800x800 pixels*

Ã—

**Figure 9**: No caption
*Dimensions: 1x1 pixels*

**Figure 10**: No caption
*Dimensions: 1x800 pixels*

**Figure 11**: No caption
*Dimensions: 1x1 pixels*

**Figure 12**: No caption
*Dimensions: 800x1 pixels*

**Figure 13**: No caption
*Dimensions: 800x800 pixels*

**Figure 14**: No caption
*Dimensions: 800x1 pixels*

**Figure 15**: No caption
*Dimensions: 1x1 pixels*

**Figure 16**: No caption
*Dimensions: 1x800 pixels*

**Figure 17**: No caption
*Dimensions: 1x1 pixels*

ğœ½ "

ğœ½ # ğš«ğœ½!

$$
Fig. 1.
**Overview of contributions (shaded):** When a kinematic model
is unavailable for the robot, 1) our method to collect curricular annotated
depth-free image data can be used instead to train 2) a high-DOFs dynamic
neural density field which the robot uses as a self-model. 3) Its forward and
inverse kinematics capabilities enable motion planning applications.
$$

$$
hinder the readiness of target applications of self-modeling in
real-world scenarios where this information is not available,
such as after damage to the robotâ€™s body during deployment.
In particular, a recent approach [ 7 ] to learn a full-body
kinematic forward model as a neural-implicit representation
requires images annotated with depth values from RGB-D
cameras. In this work, we propose to solve this obstacle by
learning neural fields in a self-supervised manner directly
from 2D images only annotated with camera parameters and
the dynamic configuration. Consequently, we approach the
task of learning a neural-implicit full-body kinematic model
from unlabeled kinematic data through training a dynamic
neural field that offers downstream compatibility (Fig. 1 ).
We achieve this by introducing a new type of *dynamic*
neural field. Previous work [ 8 , 9 , 10 ] has extended the static-
scene setup of neural radiance fields [ 1 ] by establishing time
as an additional input dimension next to 3D coordinates,
which together are mapped to density and color values. In
contrast, in this work we introduce a high number of degrees
of freedom (DOFs) that in complex interdependence change
local parts of the scene as the conditioning variables for
a coordinate-to-density map, which has not been done for
robotic applications. Different from methods using deforma-
tion from a canonical representation [ 11 ], we propose a DOF-
encoder-based dynamic neural density field, which is suitable
for modeling complex changing scenes beyond robotics.
In summary, this work contributes the following:
$$

# arXiv:2310.03624v2  [cs.CV]  19 Apr 2024

## ğŸ“‘ Page 2

$$
*â€¢* We introduce a curricular data sampling method and
neural network architecture to represent high-DOFs
object-centric scenes as dynamic neural density fields.
$$

```
*â€¢* We use our method to visually learn the first robot
self-model without depth information and from a single
camera view, and quantify its quality experimentally.
```

$$
*â€¢* Extending [ 7 ], we discuss and demonstrate downstream
applications of neural-field self-modeled kinematics in
motion planning.
$$

II. B ACKGROUND AND R ELATED W ORK

$$
**Robot self-modeling.** A self-model is a task-agnostic,
general-purpose representation of a robotâ€™s physical shape
and structure that can be acquired at any time and continually
updated without a human in the loop [ 12 , 13 ]. The objective
of enabling machines to produce a cognitive model of
themselves to guide their behavior has been inspired by
similar behavior in human beings [ 14 ]. In practice, whenever
a geometric kinematic model, which captures the spatial
relations and physical constraints of the robotâ€™s links and
joints manually as a result of simulation and engineering,
is unavailable, the ability to self-model is required. In par-
ticular, when the robotâ€™s kinematics are altered, for instance
through damage or undocumented body manipulation, the
robot can learn an updated self-model without the need to
manually re-devise the kinematic model [ 15 , 16 ].
Approaches to robot self-modeling have leveraged an-
alytical, probabilistic, and evolutionary methods [ 15 , 17 ,
16 ]. Learning-based approaches to implicitly represent self-
models were first presented in [ 18 ], necessitating training
samples that are labeled with the end effector position.
Similarly, certain approaches [ 19 , 20 ] pre-determine the set
of parameters to learn for a system, identified based on prior
knowledge about the shape or function. The most recent,
partially self-supervised approach, which constructs an ag-
nostic self-model without such information [ 7 ], still requires
depth information, which is used to learn an SDF-based
occupancy query model. In all approaches, data acquisition
plays a crucial role, with strategies ranging from entirely
random [ 7 ], to interactive [ 21 ], and targeted-exploratory
[ 19 , 22 ]. This work builds on the agnostic, neural-implicit
class of representation proposed in [ 7 ] and removes the
depth requirement using neural fields, while introducing a
curricular-random training data acquisition strategy.
**Neural (radiance) fields.** A neural field is a continuous
map from any spatial coordinate in 3D space **x** =
 *x, y, z*  *T*
$$

$$
to a scalar or vector. In neural *radiance* fields (NeRF) [ 1 ],
each point is assigned a tuple of density and color. The map
is parameterized via a neural network Î¦ , such as a multi-
layer perceptron (MLP), overfit to the specific scene,
$$

$$
*f* Î¦ : ( **x** *,* **d** ) *â†’* ( *Ïƒ,* **c** )
(1)
$$

$$
where **x** *âˆˆ* R 3  is the coordinate vector, **d** *âˆˆ* [0 *,* 1] 3 *,* *âˆ¥* **d** *âˆ¥* = 1
is the unit viewing direction, *Ïƒ* *âˆˆ* [0 *,* *âˆ* ) is the predicted
density, and **c** *âˆˆ* [0 *,* 1] 3  is the predicted RGB color. Both to
march a ray through the scene to obtain point coordinates **x**
and to compute **d** , the camera pose ****** ***w*** ***T*** ***c*** is used.
$$

$$
A NeRF is a neural-implicit representation of a scene since
novel views can be rendered by querying the learned map,
without the need to store 3D information explicitly, such
as in point clouds or voxels. From the field over the 3D
space, 2D projections to images from arbitrary camera views
are rendered via volume rendering [ 23 , 24 ]: The color of a
pixel **C** is computed by integrating the product of color,
density, and visibility of the points residing on the ray **r**
that was marched through the scene from the projection
plane within the depth view bounds. The visibility *T* *i* of a
point depends on the density values of the points between
the projection plane and that point. Using quadrature, the
integral is approximated on *N* points, which are sampled in
a stratified manner from bins on the ray, as follows:
$$

$$
Ë† **C** ( **r** ) =
$$

$$
*N*
X
$$

$$
*i* =1
Ë† *T* *i* *Î±* ( *Ïƒ* Î¦ ( **x** ( **i** ) ) *Î´* *i* ) **c** Î¦ ( **x** ( **i** ) *,* **d** )
(2)
$$

$$
Ë† *T* *i* = exp
$$

ï£«

$$
ï£­ *âˆ’*
$$

$$
*i* *âˆ’* 1
X
$$

$$
*j* =1
*Ïƒ* Î¦ ( **x** ( **j** ) ) *Î´* *j*
$$

ï£¶

ï£¸ (3)

$$
Here, **r** = **o** + *t* **d** is the pixel-corresponding ray marched
through the scene scaled by depth *t* ; *Î±* ( *Ïƒ* ) = 1 *âˆ’* exp( *âˆ’* *Ïƒ* )
maps density values into the range [0 *,* 1] ; and *Î´* *i* = *t* *i* +1 *âˆ’*
*t* *i* is the distance between adjacent points on the ray. The
differentiable nature of volume rendering allows the MLP to
be trained by minimizing a photometric reconstruction loss
between training images and renders from the same poses.
Dynamic neural fields have emerged to represent scenes
with changing components, for instance over a single time
dimension [ 8 , 9 , 10 , 25 , 26 , 27 , 28 , 29 ]. In extension,
numerous works have aimed at modeling controllable human
bodies, customarily using prior knowledge or annotations
about their shape or multi-view video training data [ 30 ,
31 , 11 , 32 , 33 , 34 ]. Related to our work, we propose a
new dynamic neural field architecture geared towards shape-
unknown objects with many DOFs that are interdependent,
trained on single-view images only.
$$

III. M ETHOD

$$
*A. High-DOFs Dynamic Neural Density Field*
$$

$$
We propose to extend neural fields to dynamic scenes in
which changes are anchored in interdependent DOFs of the
object in the scene. For this purpose, we condition the map
*f* Î¦ on the *k* -dimensional configuration of the object ***Î¸*** =
 *Î¸* 0 *, . . . , Î¸* *k* *âˆ’* 1
 *T* *âˆˆ* R *k* that causes the observed changes:
$$

$$
*f* Î¦ : ( **x** *,* ***Î¸*** ) *â†’* ( *Ïƒ* )
(4)
$$

$$
To model the shape, the field does not assign color and is
thus independent of the viewing direction. Nonetheless, color
can be included for the purpose of training the model via a
photometric loss.
Specifically, to learn a self-model of a robot, the map is
conditioned on the joint configuration of the robot composed
of *k* joint values and thus learned as a 3 + *k* -dimensional
neural field. We can subsequently compute density fields in
novel configurations and render these from novel views.
$$

## ğŸ“‘ Page 3

**Figure 1**: No caption
*Dimensions: 1028x887 pixels*

**Figure 2**: No caption
*Dimensions: 800x800 pixels*

$$
ğ‘ª ğ’‘ğ’“ğ’ğ’‹ (ğ‘¿ ğ’‘ğ’“ğ’ğ’‹ |ğœ½ ğŸ ) =
$$

**Figure 3**: No caption
*Dimensions: 800x800 pixels*

$$
ğ‘ª ğ’‘ğ’“ğ’ğ’‹ (ğ‘¿ ğ’‘ğ’“ğ’ğ’‹ |ğœ½ ğŸ ) =
$$

$$
*joints config* ğœ½
$$

$$
- value ğœƒ !
$$

â€¦

$$
- value ğœƒ "
$$

Î¦ '() !"

Î¦ '() ğ’™

Î¦ '() ğœ½

$$
**Variable**
**encoders**
$$

$$
**Group**
**encoders**
$$

$$
**Density**
$$

$$
**MLP**
**Inputs**
$$

Î¦ '() !% Î¦ '() !&

Î¦ '() '"

Î¦ '() '(

$$
Î¦ *+,
$$

$$
**con$ig** **** ğœ½ ğŸ
$$

$$
**config** ğœ½ ğŸ
$$

$$
ğ¿(ğ‘ª ğ’‘ğ’“ğ’ğ’‹ , ğ‘ª +  )
$$

x

y

z

ğ’„

ğœ ğ‘„(ğœ½ ğ’ğ’†ğ’˜ )

**Figure 4**: No caption
*Dimensions: 1028x748 pixels*

$$
**config** ğœ½ ğ’ğ’†ğ’˜ğŸ
$$

$$
*point* ğ’™
$$

$$
- coord. ğ‘¥
$$

$$
- coord. ğ‘¦
$$

$$
- coord. ğ‘§
$$

$$
**A**
$$

$$
**config** ğœ½ ğ’ğ’†ğ’˜ğŸ
$$

â€¦

$$
**Neural Rendering**
$$

$$
**(loss only)**
$$

ğ‘£ğ‘œğ‘™ğ‘¢ğ‘šğ‘’ ğ‘Ÿğ‘’ğ‘›ğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘”

$$
**Application**
$$

$$
ğ‘ª $
$$

ray marching inputs to NN inputs to loss differentiable operation backpropagation application

âŠ•

z

x y z

x y

$$
Q(ğœ½ ğ’ğ’†ğ’˜ğŸ ) = 0
Q(ğœ½ ğ’ğ’†ğ’˜ğŸ ) = 1
$$

ğ‘“ !

$$
**C**
$$

$$
**B**
$$

âŠ•

âŠ•

âŠ•

**Figure 5**: No caption
*Dimensions: 1028x167 pixels*

$$
Fig. 2.
**Overview of proposed method:** A) Training images of robot in different configurations: While the point coordinates and annotated configuration
are the inputs to the neural network, the true color of the pixel is used for the reconstruction loss. B) Neural network architecture: The DOF values and
spatial coordinates are individually encoded, concatenated and group-wise encoded, and concatenated and processed to an output density. C) The trained
neural density field is used to evaluate the validity of a configuration relative to an obstacle by predicting the densities of points queried from its volume.
$$

$$
**Encoder-based architecture.** We parameterize the map
via a neural network based on [ 1 ], that is an MLP with ReLU
activations. Extending this architecture by adding ***Î¸*** as input
parameters produces unsatisfactory results. Consequently, we
extend [ 7 ]â€™s approach and introduce separate encoders for the
spatial and conditioning input variables. This leverages the
independence of the coordinates and the DOFs configuration
and promises to learn useful representations resulting from
the combinations of the constituents of each group.
Applying the shuffled curriculum learning approach in-
troduced below, however, results in continual forgetting of
previously learned relationships of DOFs as the training
progresses, reducing performance on previously well re-
constructed samples. For this reason, we introduce DOF-
individual encoders, MLPs that encode each input variable.
Since gradients flowing back to the weights of these MLPs
will be zero when the joint values are zero in batches in
which exclusively other DOFs are sampled, we argue this
improves the memorability of useful features for the behavior
introduced by each DOF individually. The outputs of the
individual encoders are concatenated and chained to the
group-based encoders. The complete model is described by:
$$

$$
*f* Î¦ ( **x** *,* ***Î¸*** ) =
$$

Î¦ MLP

$$
Î¦ Enc **x** (
$$

2 M

$$
*l* =0
Î¦ Enc *xl* ( *x* *l* )) *,* Î¦ Enc ***Î¸*** (
$$

$$
*k* *âˆ’* 1
M
$$

$$
*l* =0
Î¦ Enc *Î¸l* ( *Î¸* *l* ))
$$

!

(5)

$$
where *âŠ•* is the concatenation operator and Î¦( *x* ) = *h* *n* *â—¦*
$$

$$
*. . .* *â—¦* *h* 1 ( *x* ) is an *n* -layer MLP with ReLU activations. The
architecture is shown in Fig. 2 .
Following [ 1 ], we train two models of this type to sepa-
rately model spatially coarse and fine predictions. The second
model evaluates points on the ray sampled from regions
of *t* where the first model has resulted in higher density
predictions. All points are used to render the projection of a
ray. We find the sinusoidal positional encoding used in [ 1 ] to
hinder the learning of the physical movement associated with
traversing the DOF value ranges. Consequently, we substitute
it with the [ *âˆ’* 1 *,* 1] -normalized original joint values, resulting
in 3 + *k* -dimensional inputs to the model.
**Learning from a one-camera setup.** To circumvent the
need for multiple cameras observing the robot to produce the
neural field, which limits real-world applications, we harness
the mobility of the robotâ€™s base. Given the robotâ€™s configu-
ration ***Î¸*** and the camera pose as camera-to-world transform
***w*** ***T*** ***c*** , we enforce the first DOF to be the base rotation. For a
front-facing camera pointing at the center of the robot with
zero pitch and roll, rotating the object at its base is equivalent
to rotating the camera about the upward axis. Hence, multi-
view consistency for the density predictions can be enabled
by assigning ( ***w*** ***T*** ***c*** ) *â€²* ** *â†* ***R*** ***z*** ( *Î¸* 0 ) ***w*** ***T*** ***c*** and *Î¸* *â€²*
0 ** *â†* 0 .
**Curricular training data.** Due to the large space of
configurations and the serial dependence among the *k* DOFs,
the most distal jointâ€™s position depending on all *k* *âˆ’* 1 previous
DOFs, learning a high-DOFs neural field is difficult. Thus,
the training data generation approach is crucial to the success
$$

## ğŸ“‘ Page 4

$$
of the model inferring the correct marginal influence of each
DOF. We propose a curriculum-learning-inspired sampling
approach. For the set of all DOF indices Î˜ = *{* *l* *}* *k* *âˆ’* 1
*l* =1  , we
compute the powerset containing all subsets of Î˜ and sort it
in ascending order by magnitude, excluding the empty set:
*S* Î˜ = *{* *s* *}* *s* *âŠ†* Î˜ . For each set of DOF indices *s* *âˆˆ* *S* Î˜ , samples
are generated by uniformly randomly sampling values from
the permissible ranges of the DOFs in *s* . The values of
the remaining DOFs are fixed to zero. Thus, we encourage
learning the contribution of each DOF first by itself and
then in combination with other DOFs in order of increasing
complexity, until all DOFs are interacting. For example,
$$

$$
***Î¸*** **(1** ***,*** **4)**  =
 0
*Î¸* 1 *âˆ¼* *D* 1
0
0
*Î¸* 4 *âˆ¼* *D* 4
*. . .*
0 
*,*
(6)
$$

$$
where *D* *i* = *U* ( *Î¸* ( *min* )
*i*
*, Î¸* ( *max* )
*i*
) . We find shuffling the images
such that images with different numbers of active DOFs lie
in the same batch to improve the training performance.
**Training.** We optimize the model via a photometric mean
squared error (MSE) loss between ground-truth **C** **proj**  and
rendered pixels **** **Ë†** **C** . Our experiments suggest that keeping the
RGB output improves training performance. Nonetheless, the
density prediction is the only output kept to be used in the
self-model after training. To train density-output-only high-
DOFs neural fields, the MSE loss may be used between
binarized images and renderings with **c** set to black.
$$

$$
*B. Neural-Field Self-Model and Applications*
**Self-model.** The trained map *f* Î¦ is an implicit full-body
kinematic model of the robot since it enables the recon-
struction of its shape conditioned on its joint configuration.
Learning this model only from annotated 2D images replaces
the need to know the robot geometry altogether.
**Motion planning: Reaching a target via inverse kine-**
**matics.** Extending [ 7 ], we demonstrate motion planning as an
inherent downstream application of the model. Due to the dif-
ferentiable forward prediction of the density of a point given
the configuration, we can compute the inverse kinematics,
that is the configuration such that a point is occupied. For this
purpose, the MLP parameters are fixed, and the input joint
values are optimized via projected gradient descent (PGD)
to minimize the delta to the desired density.
By choosing appropriate points, the robot can, for ex-
ample, compute how to reach an object. Furthermore, by
selecting the initial configuration of the optimization to be the
current configuration of the robot and acting in an obstacle-
free environment, the inverse kinematics optimization steps
can be cast as a path to reach the target. Precisely, given
information about the target, we uniformly sample *N* points
from its surface *O* *s* = *{* **x** ( **i** ) *}* *N*
*i* =1  and query the robotâ€™s
density on them, leveraging that density on the surface above
a threshold *Ï„* indicates touch. In the fine model, starting from
a no-touch configuration ***Î¸*** **(0)** , we minimize the following
loss, which will be *â‰¤* 0 when the target is reached:
$$

$$
*L* ( ***Î¸*** *, O* *s* ) = min
**x** *âˆˆ* *O* *s*  [ *âˆ’* *Î±* ( *f* Î¦ ( **x** *,* ***Î¸*** ))] + *Ï„*
(7)
$$

$$
The final ReLU activation at Î¦ MLP â€™s output unit for *Ïƒ* is
removed to produce non-zero gradients. To enforce that the
$$

$$
final joint configuration and every step of the optimization
are within the joint limits, after each step of size *Î·* the
joint values are projected back into the *k* -dimensional ball
representing the permissible ranges:
$$

$$
***Î¸*** **(** ***j*** **+1)**  = Î  ***Î¸*** **(** ***max*** **)**
$$

$$
***Î¸*** **(** ***min*** **)**
$$

$$

***Î¸*** **(** ***j*** **)** ** *âˆ’* *Î·* *âˆ‚L* ( ***Î¸*** **(** ***j*** **)** *, O* *s* )
$$

$$
*âˆ‚* ***Î¸*** **(** ***j*** **)**
$$

 (8)

$$
If only the inverse kinematics task is required, random initial-
izations of the configuration can accelerate the optimization.
**Motion planning: Configuration space.** For more com-
plex constraints and in the presence of obstacles, customary
motion planning algorithms can be used with the self-model.
Any planning algorithm using the configuration space, that
is the binary map over the *k* -dimensional space of possible
configurations indicating which configuration is collision-
free, is compatible with the implicit kinematic model. Given
the neural density field and information about obstacle(s) in
the scene, a configuration is valid if the maximum density
of the robot among *N* uniformly sampled points from the
volume of the obstacle *O* *v* is below a threshold. Thus,
sampling-based motion planning methods that search the
configuration space, such as Probabilistic Roadmap [ 36 ] or
Rapidly-exploring Random Trees (RRT) [ 37 ], can be used.
The membership in the configuration space is queried as:
$$

$$
*Q* ( ***Î¸*** ) =
$$

$$
(
*True*
if: max **x** *âˆˆ* *O* *v* [ *Î±* ( *f* Î¦ ( **x** *,* ***Î¸*** ))] *< Ï„*
*False*
else.
(9)
$$

IV. E XPERIMENTAL S ETUP

$$
We demonstrate our method on a simulated 7-DOF robot.
**Training distribution.** We apply the curriculum data
generation described above with 16 different configurations
per set *s* and 6 random base rotations sampled anew for each
configuration, totalling 5 *,* 588 annotated 400 *Ã—* 400 images.
We generate the images in simulation of the Panda robot [ 38 ]
with 7 joints and a rotatable base ( *k* = 8 ), using the Pybullet
simulator [ 39 ]. We group batches of 15 images and, to avoid
overfitting to one batch, only process 10 *,* 240 rays per image.
**Training.** We use the described architecture with 3-layer
DOF-individual encoder MLPs, 1-layer coordinate encoder
MLPs, 2-layer group encoder MLPs, and the final 7-layer
density MLP. We train using the Adam optimizer for
1,320,000 steps with a learning rate of 4 *e* *âˆ’* 5 and optimize
the parameters of all MLPs together.
**Visualizations.** To visualize the *predicted* self-model, we
produce point clouds by querying the field from two camera
poses at the front and side of the scene on the *y* - and
*x* -axes. Points with alpha values above 0 *.* 015 are kept,
determining the isolevel. On the fused point cloud, marching
cubes [ 40 ] reconstruction is applied to generate a triangle
mesh, followed by hole repair and Taubin smoothing [ 41 ]
algorithms. To visualize the *ground truth* , we simulate the
true robot model in Pybullet. The ground-truth point cloud
for a joint configuration is the fusion of six point clouds
from RGB-D images obtained from two camera views per
axis, one at either end, with the view centered at the object.
The mesh is then produced identically to the predicted mesh.
$$

## ğŸ“‘ Page 5

$$
TABLE I
S PATIAL DISTANCES BETWEEN PREDICTED AND GROUND - TRUTH SELF - MODEL MESHES IN RANDOM TEST CONFIGURATIONS AND ACROSS TEST SET .
$$

$$
Distance metric
config a
config b
config c
config d
config e
test set (n=30)
Chamfer-L2 (m) *â†“*
.017
.019
.053
.013
.013
.024
Chamfer-L2 (% of workspace- *z* ) *â†“*
1.35
1.51
4.22
1.06
1.07
1.94
Surface area IoU *â†‘*
.501
.479
.408
.571
.572
.496
Hull volume IoU *â†‘*
.685
.607
.336
.714
.690
.573
$$

$$
**Ground**
**truth**
$$

$$
**Prediction**
front view
$$

top view

top view

front view

$$
ğœ½ ğ’‚ = [0.0, âˆ’1.77, âˆ’1.62,
2.36, âˆ’0.62, âˆ’1.33, 0.0, 0.89]
$$

$$
ğœ½ ğ’ƒ = [0.0, âˆ’0.89, 1.26, 2.36,
âˆ’0.62, âˆ’1.92, 2.47, âˆ’1.77]
$$

$$
ğœ½ ğ’„ = [0.0, âˆ’2.80, âˆ’1.62, âˆ’1.77,
âˆ’1.24, âˆ’2.80, 2.28, âˆ’0.30]
$$

$$
ğœ½ ğ’… = [0.0, âˆ’1.48, 0, âˆ’0.59,
âˆ’2.48, âˆ’1.33, 1.33, 0.89]
$$

$$
ğœ½ ğ’† = [0.0, âˆ’1.92, 0.18, 0.59,
âˆ’1.24, âˆ’2.95, 0.76, 0.89]
$$

**Figure 1**: No caption
*Dimensions: 960x960 pixels*

**Figure 2**: No caption
*Dimensions: 960x960 pixels*

**Figure 3**: No caption
*Dimensions: 960x960 pixels*

**Figure 4**: No caption
*Dimensions: 960x960 pixels*

**Figure 5**: No caption
*Dimensions: 960x960 pixels*

**Figure 6**: No caption
*Dimensions: 960x960 pixels*

**Figure 7**: No caption
*Dimensions: 960x960 pixels*

**Figure 8**: No caption
*Dimensions: 960x960 pixels*

**Figure 9**: No caption
*Dimensions: 960x960 pixels*

**Figure 10**: No caption
*Dimensions: 960x960 pixels*

**Figure 11**: No caption
*Dimensions: 960x960 pixels*

**Figure 12**: No caption
*Dimensions: 960x960 pixels*

**Figure 13**: No caption
*Dimensions: 960x960 pixels*

**Figure 14**: No caption
*Dimensions: 960x960 pixels*

**Figure 15**: No caption
*Dimensions: 960x960 pixels*

**Figure 16**: No caption
*Dimensions: 760x428 pixels*

**Figure 17**: No caption
*Dimensions: 960x960 pixels*

**Figure 18**: No caption
*Dimensions: 960x960 pixels*

**Figure 19**: No caption
*Dimensions: 960x960 pixels*

**Figure 20**: No caption
*Dimensions: 960x960 pixels*

```
Fig. 3.
**Self-model results:** Predicted vs. ground-truth meshes, smoothed and reconstructed via marching cubes from point clouds generated by querying
the high-DOFs neural density field in the given random test configuration. The configurations are shown in radians. Please also see suppl. video [ 35 ].
```

$$
**Metrics.** To assess the quality of our model, we compare
the ground-truth against the predicted meshes. First, we use
the customary Chamfer-L2 distance, the shortest Euclidean
distance of a point in a set to any point in the other set,
applied symmetrically and averaged over all points. This
returns an average spatial offset per point. We generate the
point sets by sampling uniformly from the mesh surfaces.
Second, as measures for the spatial similarity of the shapes,
we compute two intersection over union (IoU) metrics. Both
are based on a union point cloud, constructed by fusion, and
an intersection point cloud, constructed by keeping points
with a negative signed distance to the mesh defined by the
other point cloud. We compute a 2D metric, relating the
surface areas of the meshes reconstructed from the point
clouds, and a 3D metric, relating the volumes of their convex
hulls, produced from uniformly sampled surface points.
$$

V. R ESULTS

```
**Neural-field self-model.** We show the predicted meshes
from the 7-DOF robot self-model for five random test
configurations from a fixed view in Fig. 3 . It can be observed
```

$$
that in each configuration, the prediction follows the shape of
the ground truth, subject to small deviations in the rotations
of smaller parts of the body. For the shown samples, this
indicates that the model learned to correctly approximate
the shape from the configuration, despite the large space of
possible configurations. We find that density scales with the
certainty in the prediction and that despite the solid material
of the robot, most of the non-zero density values are in the
lower regime as opposed to close to one. Consequently, the
threshold selection, that is the marching cube isolevel, is a
significant hyperparameter since it controls the sensitivity
with which sampled points are included. A too high value
may exclude parts of the body in whose prediction the model
is less certain. Due to the serial dependence among the DOFs
- the first linkâ€™s density only depends on the first joint value,
whereas the seventh linkâ€™s density depends on all previous
joint values - those excluded parts are the upper parts of the
body. The highest density values belong to points in the base
of the robot, which remains static. In addition, the querying
resolution determines the trade-off between computational
cost and approximation of the true predicted model.
$$

## ğŸ“‘ Page 6

Step 0 (start) Step 53 (loss<0)

$$
**Predicted**
**Trajectory 1,2**
$$

Step 0 (start) Step 239 (loss<0) Step 59

top view

front view

**Figure 1**: No caption
*Dimensions: 960x960 pixels*

**Figure 2**: No caption
*Dimensions: 960x960 pixels*

**Figure 3**: No caption
*Dimensions: 960x960 pixels*

**Figure 4**: No caption
*Dimensions: 960x960 pixels*

**Figure 5**: No caption
*Dimensions: 960x960 pixels*

**Figure 6**: No caption
*Dimensions: 960x960 pixels*

**Figure 7**: No caption
*Dimensions: 960x960 pixels*

**Figure 8**: No caption
*Dimensions: 960x960 pixels*

**Figure 9**: No caption
*Dimensions: 960x960 pixels*

**Figure 10**: No caption
*Dimensions: 960x960 pixels*

**Figure 11**: No caption
*Dimensions: 960x960 pixels*

**Figure 12**: No caption
*Dimensions: 960x960 pixels*

**Figure 13**: No caption
*Dimensions: 960x960 pixels*

**Figure 14**: No caption
*Dimensions: 960x960 pixels*

**Figure 15**: No caption
*Dimensions: 960x960 pixels*

**Figure 16**: No caption
*Dimensions: 793x529 pixels*

**Figure 17**: No caption
*Dimensions: 793x529 pixels*

**Figure 18**: No caption
*Dimensions: 793x529 pixels*

**Figure 19**: No caption
*Dimensions: 793x529 pixels*

**Figure 20**: No caption
*Dimensions: 793x529 pixels*

$$
**A**
$$

Step 0 (start) Step 5 Step 8 Step 18 (goal) Step 10

$$
**Predicted**
**Trajectory 3**
$$

$$
**B**
$$

top view

front view

$$
Fig. 4.
**Motion planning results:** A) Joint value optimization via input PGD. A density loss is minimized when the sphere (green) is touched. B) RRT
planning in config. space to intersect a point (green). The query model rejects samples with density on the obstacle (red). Please also see suppl. video [ 35 ].
$$

$$
In addition to the qualitative evaluation, numerical results
on the spatial distance metrics are provided in Table I . As
the most important metric, the mean of the Chamfer-L2
distance for the test set is 1.94% relative to the length of the
shortest dimension of the workspace of the robot, 1.254m
along the vertical axis. This indicates that, on average, each
point on the mesh that was reconstructed from the points
in the volume predicted to have sufficiently high density is
close to a point on the robotâ€™s true surface given the queried
configuration. Greater variance can be observed for the two
IoU metrics. For the volume-based IoU, this is due to the
constraint that the hull must convexly contain all points of
the surface point cloud so that outliers have a large effect on
its shape and, thus, volume. In addition, while marginally off-
positioned predicted robot parts can still produce moderate
Chamfer-L2 distances, these parts may not or only partially
intersect with the ground-truth parts, leading to a lower value.
The surface area is similarly outlier-sensitive and depends on
the smoothness of the surface, which is not reliably given.
**Motion planning.** In Fig. 4 , PGD- and RRT-generated
trajectories are shown. The task for the former was to touch
an object in an obstacle-free environment, while the task for
the latter was to circumvent an obstacle to move from a start
to a goal position. For PGD, the joint-limit-projected opti-
mization results in valid trajectory steps. The robot moves
itself into a configuration in which the sphere is touched such
that the density on a surface point is above the threshold
( *Ï„* = 0 *.* 6 ). Unlike in classical kinematics, the part touching
the target can be different from the end effector. However,
distant start configurations may stop in local optima before
$$

$$
reaching the target, rendering the learning rate a crucial
hyperparameter. In addition, *Ï„* controls the closeness to the
target in the final configuration. In those challenging cases,
neural-field-based RRT reliably finds valid trajectories. In
Fig. 4 , the robot is able to move around the obstacle to
reach its goal position. This approach is computationally
more expensive since the neural field is queried extensively
to construct the tree. Similarly, the strategy for sampling from
the obstacleâ€™s volume impacts the performance and runtime.
$$

$$
VI. C ONCLUSION AND O UTLOOK
We propose dynamic neural density fields conditioned on
high DOFs. To this end, we introduce a hierarchical MLP
architecture and curricular data sampling strategy. We use
this method to learn the first neural-implicit self-model of a
robot without depth or geometry information and from one
camera, which can be used in lieu of a classical kinematic
model. Future work may explore limiting the training data to
more sparsely observed DOFs configurations and removing
the need for camera parameter annotation via automatic esti-
mation. In navigation and manipulation tasks, the integration
of our approach, which models the robot, with previous work,
which models the robotâ€™s environment, would be beneficial,
as well as an extension to multi-robot setups. We highlight
the usability of our method for dynamic object-centric scenes
outside robotics in general DOFs-controlled environments.
$$

$$
VII. A CKNOWLEDGMENT
This work was supported in part by the US National Sci-
ence Foundation (NSF) AI Institute for Dynamical Systems
(DynamicsAI.org), grant 2112085.
$$

## ğŸ“‘ Page 7

$$
R EFERENCES
[1]
B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-
mamoorthi, and R. Ng, â€œNerf: Representing scenes as neural radiance
fields for view synthesis,â€ in *European Conference on Computer*
*Vision* , Springer, 2020, pp. 405â€“421.
[2]
S. Lee, L. Chen, J. Wang, A. Liniger, S. Kumar, and F. Yu, â€œUncer-
tainty guided policy for active robotic 3d reconstruction using neural
radiance fields,â€ *IEEE Robotics and Automation Letters* , vol. 7, no. 4,
pp. 12 070â€“12 077, 2022.
[3]
M. Adamkiewicz, T. Chen, A. Caccavale, R. Gardner, P. Culbertson,
J. Bohg, and M. Schwager, â€œVision-only robot navigation in a neural
radiance world,â€ *IEEE Robotics and Automation Letters* , vol. 7, no. 2,
pp. 4606â€“4613, 2022.
[4]
A. Moreau, N. Piasco, D. Tsishkou, B. Stanciulescu, and A. d. L.
Fortelle, â€œLens: Localization enhanced by nerf synthesis,â€ in *Pro-*
*ceedings of the 5th Conference on Robot Learning* , A. Faust, D.
Hsu, and G. Neumann, Eds., ser. Proceedings of Machine Learning
Research, vol. 164, PMLR, Nov. 2022, pp. 1347â€“1356.
[5]
D. Maggio, M. Abate, J. Shi, C. Mario, and L. Carlone, â€œLoc-nerf:
Monte carlo localization using neural radiance fields,â€ in *2023 IEEE*
*International Conference on Robotics and Automation (ICRA)* , IEEE,
2023, pp. 4018â€“4025.
[6]
H. W. Stone, *Kinematic modeling, identification, and control of*
*robotic manipulators* . Springer Science & Business Media, 1987,
vol. 29.
[7]
B. Chen, R. Kwiatkowski, C. Vondrick, and H. Lipson, â€œFully
body visual self-modeling of robot morphologies,â€ *Science Robotics* ,
vol. 7, no. 68, eabn1944, 2022.
[8]
A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, â€œD-
nerf: Neural radiance fields for dynamic scenes,â€ in *Proceedings*
*of the IEEE/CVF Conference on Computer Vision and Pattern*
*Recognition* , 2021, pp. 10 318â€“10 327.
[9]
K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M.
Seitz, and R. Martin-Brualla, â€œNerfies: Deformable neural radiance
fields,â€ in *Proceedings of the IEEE/CVF International Conference*
*on Computer Vision* , 2021, pp. 5865â€“5874.
[10]
E. Tretschk, A. Tewari, V. Golyanik, M. ZollhÂ¨ofer, C. Lassner, and
C. Theobalt, â€œNon-rigid neural radiance fields: Reconstruction and
novel view synthesis of a dynamic scene from monocular video,â€ in
*Proceedings of the IEEE/CVF International Conference on Computer*
*Vision* , 2021, pp. 12 959â€“12 970.
[11]
S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao,
â€œAnimatable neural radiance fields for modeling dynamic human
bodies,â€ in *Proceedings of the IEEE/CVF International Conference*
*on Computer Vision* , 2021, pp. 14 314â€“14 323.
[12]
R. Kwiatkowski, *Deep Self-Modeling for Robotic Systems* . Columbia
University, 2022.
[13]
A. Dearden, â€œDevelopmental learning of internal models for
robotics,â€ Ph.D. dissertation, Imperial College London, 2008.
[14]
P. Rochat, â€œFive levels of self-awareness as they unfold early in life,â€
*Consciousness and cognition* , vol. 12, no. 4, pp. 717â€“731, 2003.
[15]
J. Bongard, V. Zykov, and H. Lipson, â€œResilient machines through
continuous self-modeling,â€ *Science* , vol. 314, no. 5802, pp. 1118â€“
1121, 2006.
[16]
J. C. Bongard and H. Lipson, â€œAutomated damage diagnosis and
recovery for remote robotics,â€ in *IEEE International Conference on*
*Robotics and Automation, 2004. Proceedings. ICRAâ€™04. 2004* , IEEE,
vol. 4, 2004, pp. 3545â€“3550.
[17]
K. Gold and B. Scassellati, â€œUsing probabilistic reasoning over time
to self-recognize,â€ *Robotics and Autonomous Systems* , vol. 57, no. 4,
pp. 384â€“392, 2009.
[18]
R. Kwiatkowski and H. Lipson, â€œTask-agnostic self-modeling ma-
chines,â€ *Science Robotics* , vol. 4, no. 26, eaau9354, 2019.
[19]
K. Hang, W. G. Bircher, A. S. Morgan, and A. M. Dollar, â€œMa-
nipulation for self-identification, and self-identification for better
manipulation,â€ *Science Robotics* , vol. 6, no. 54, eabe1321, 2021.
[20]
Z. Jiang, W. Zhou, H. Li, Y. Mo, W. Ni, and Q. Huang, â€œA new
kind of accurate calibration method for robotic kinematic parameters
based on the extended kalman and particle filter algorithm,â€ *IEEE*
*Transactions on Industrial Electronics* , vol. 65, no. 4, pp. 3337â€“3345,
2017.
[21]
J. Bohg, K. Hausman, B. Sankaran, O. Brock, D. Kragic, S. Schaal,
and G. S. Sukhatme, â€œInteractive perception: Leveraging action in
perception and perception in action,â€ *IEEE Transactions on Robotics* ,
vol. 33, no. 6, pp. 1273â€“1291, 2017.
$$

$$
[22]
B. Amos *et al.* , â€œLearning awareness models,â€ in *International*
*Conference on Learning Representations* , 2018.
[23]
J. T. Kajiya and B. P. Von Herzen, â€œRay tracing volume densities,â€
*ACM SIGGRAPH computer graphics* , vol. 18, no. 3, pp. 165â€“174,
1984.
[24]
N. Max, â€œOptical models for direct volume rendering,â€ *IEEE Trans-*
*actions on Visualization and Computer Graphics* , vol. 1, no. 2,
pp. 99â€“108, 1995.
[25]
Z. Li, S. Niklaus, N. Snavely, and O. Wang, â€œNeural scene flow fields
for space-time view synthesis of dynamic scenes,â€ in *Proceedings*
*of the IEEE/CVF Conference on Computer Vision and Pattern*
*Recognition* , 2021, pp. 6498â€“6508.
[26]
W. Xian, J.-B. Huang, J. Kopf, and C. Kim, â€œSpace-time neural
irradiance fields for free-viewpoint video,â€ in *Proceedings of the*
*IEEE/CVF Conference on Computer Vision and Pattern Recognition* ,
2021, pp. 9421â€“9431.
[27]
C. Gao, A. Saraf, J. Kopf, and J.-B. Huang, â€œDynamic view synthesis
from dynamic monocular video,â€ in *Proceedings of the IEEE/CVF*
*International Conference on Computer Vision* , 2021, pp. 5712â€“5721.
[28]
A. Noguchi, X. Sun, S. Lin, and T. Harada, â€œNeural articulated
radiance field,â€ in *Proceedings of the IEEE/CVF International Con-*
*ference on Computer Vision* , 2021, pp. 5762â€“5772.
[29]
T. Li *et al.* , â€œNeural 3d video synthesis from multi-view video,â€ in
*Proceedings of the IEEE/CVF Conference on Computer Vision and*
*Pattern Recognition* , 2022, pp. 5521â€“5531.
[30]
H. A. Correia and J. H. Brito, â€œ3d reconstruction of human bodies
from single-view and multi-view images: A systematic review,â€
*Computer Methods and Programs in Biomedicine* , p. 107 620, 2023.
[31]
M. Sun, D. Yang, D. Kou, Y. Jiang, W. Shan, Z. Yan, and L. Zhang,
â€œHuman 3d avatar modeling with implicit neural representation:
A brief survey,â€ in *2022 14th International Conference on Signal*
*Processing Systems (ICSPS)* , IEEE, 2022, pp. 818â€“827.
[32]
L. Liu, M. Habermann, V. Rudnev, K. Sarkar, J. Gu, and C. Theobalt,
â€œNeural actor: Neural free-view synthesis of human actors with pose
control,â€ *ACM transactions on graphics (TOG)* , vol. 40, no. 6, pp. 1â€“
16, 2021.
[33]
C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I.
Kemelmacher-Shlizerman, â€œHumannerf: Free-viewpoint rendering
of moving people from monocular video,â€ in *Proceedings of the*
*IEEE/CVF conference on computer vision and pattern Recognition* ,
2022, pp. 16 210â€“16 220.
[34]
R. Li, J. Tanke, M. Vo, M. ZollhÂ¨ofer, J. Gall, A. Kanazawa, and
C. Lassner, â€œTava: Template-free animatable volumetric actors,â€ in
*European Conference on Computer Vision* , Springer, 2022, pp. 419â€“
436.
[35]
https://youtu.be/c1G0zFrM_sQ .
[36]
L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Overmars,
â€œProbabilistic roadmaps for path planning in high-dimensional con-
figuration spaces,â€ *IEEE transactions on Robotics and Automation* ,
vol. 12, no. 4, pp. 566â€“580, 1996.
[37]
S. LaValle, â€œRapidly-exploring random trees: A new tool for path
planning,â€ *Research Report 9811* , 1998.
[38]
E. Coumans, *Pybullet robots* , 2020. [Online]. Available: https:
//github.com/erwincoumans/pybullet_robots .
[39]
E. Coumans and Y. Bai, â€œPybullet, a python module for physics
simulation for games, robotics and machine learning,â€ 2016.
[40]
T. Lewiner, H. Lopes, A. W. Vieira, and G. Tavares, â€œEfficient im-
plementation of marching cubesâ€™ cases with topological guarantees,â€
*Journal of graphics tools* , vol. 8, no. 2, pp. 1â€“15, 2003.
[41]
G. Taubin, â€œCurve and surface smoothing without shrinkage,â€ in
*Proceedings of IEEE international conference on computer vision* ,
IEEE, 1995, pp. 852â€“857.
$$


---

## ğŸ¤– Claude Code Analysis Tips
- Use Ctrl+F to quickly find specific algorithms or methods
- Mathematical formulas are in LaTeX format for easy copying
- Code blocks are clearly marked for implementation reference
- Tables and figures are structurally annotated
